[TOC]



## 1、kubernetes基本概念

kubernetes中 node，pod，replication，controller，service等都可以看作一种资源对象，几乎所有的资源对象都可以通过kubectl工具执行增删改查等操作。并将其保存在etcd中持久化存储。可以看作kubernetes是一个自动化的资源控制系统，他通过ETCD保存的 “资源期望状态”和当前环境的 “实际资源状态”的差异来实现自动控制和自动纠错的。

**kubernetes集群的二种管理角色：master和node**

**==Master 	是指集群的控制节点==**  每个集群都需要一个master节点来负责整个集群的管理和控制，基本上kubernetes的所有控制命令都会发给他，他来负责具体的执行过程，一般执行命令都是在master节点上执行的，master节点要占用一个独立的服务器(高可用部署建议用3台机器)。如果master宕机或者不可用，集群内的容器都会失效。

**Master节点上运行着以下关键进程：**

- kubernetes API server :(kube-apiserver) 提供了http rest接口的关键服务进程，是kubernetes里所有资源的增删改查等操作的唯一接口。并提供认证、授权、访问控制、API注册和发现等机制
- kubernetes Controller Manager: (kube-controller-manager) 所有资源对象的自动化控制中心，可以理解为资源对象的 大总管。维护集群的状态，比如故障检测、自动扩展、滚动更新等
- Kubernetes Scheduler :(kube-scheduler) 负责资源调度（pod调度）的进程，相当于公交公司的调度室。按照预定的调度策略将 Pod 调度到相应的机器上
- master节点上还需要启动一个etcd服务，所有的资源对象的数据都保存在etcd中。

| Protocol   | Direction | Port Range  | Purpose                          |
| ---------- | --------- | ----------- | -------------------------------- |
| TCP        | Inbound   | 6443*       | Kubernetes API server            |
| TCP        | Inbound   | 8080        | Kubernetes API insecure server   |
| TCP        | Inbound   | 2379-2380   | etcd server client API           |
| TCP        | Inbound   | 10250       | Kubelet API                      |
| TCP        | Inbound   | 10251       | kube-scheduler healthz           |
| TCP        | Inbound   | 10252       | kube-controller-manager healthz  |
| TCP        | Inbound   | 10253       | cloud-controller-manager healthz |
| TCP        | Inbound   | 10255       | Read-only Kubelet API            |
| TCP        | Inbound   | 10256       | kube-proxy healthz               |
| ---------- |           | node节点    |                                  |
| TCP        | Inbound   | 4194        | Kubelet cAdvisor                 |
| TCP        | Inbound   | 10248       | Kubelet healthz                  |
| TCP        | Inbound   | 10249       | kube-proxy metrics               |
| TCP        | Inbound   | 10250       | Kubelet API                      |
| TCP        | Inbound   | 10255       | Read-only Kubelet API            |
| TCP        | Inbound   | 10256       | kube-proxy healthz               |
| TCP        | Inbound   | 30000-32767 | NodePort Services**              |

**==Node    集群中的机器都成为node节点==** 是kubernetes集群中的工作负载节点。每个node都会被master分配一些docker容器，当某个node宕机后，其他的docker容器会迁移到其他node节点上。

**Node节点上运行着以下关键进程**

- kubelet：维持容器的生命周期，负责pod的创建，启动 暂停等任务。同时也负责 Volume（CVI）和网络（CNI）的管理
- kube-proxy：实现与kubernetes service的通讯，和负载均衡机制的重要组件。负责为 Service 提供 cluster 内部的服务发现和负载均衡
- docker engine: docker引擎，负责本机的容器创建和管理工作。

==除了核心组件，还有一些推荐的 Add-ons：==

kube-dns 负责为整个集群提供 DNS 服务
- Ingress Controller 为服务提供外网入口
- Heapster 提供资源监控
- Dashboard 提供 GUI
- Federation 提供跨可用区的集群
- Fluentd-elasticsearch 提供集群日志采集、存储与查询

**==Container（容器）==**是一种便携式、轻量级的操作系统级虚拟化技术。它使用namespace 隔离不同的软件运行环境，并通过镜像自包含软件的运行环境，从而使得容器可以很方便的在任何地方运行。由于容器体积小且启动快，因此可以在每个容器镜像中打包一个应用程序。这种一对一的应用镜像关系拥有很多好处。使用容器，不需要与外部的基础架构环境绑定, 因为每一个应用程序都不需要外部依赖，更不需要与外部的基础架构环境依赖。完美解决了从开发到生产环境的一致性问题。

容器同样比虚拟机更加透明，这有助于监测和管理。尤其是容器进程的生命周期由基础设施管理，而不是被进程管理器隐藏在容器内部。最后，每个应用程序用容器封装，管理容器部署就等同于管理应用程序部署。

**==Pod==**  Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。

每个pod都有一个特殊的pause容器和一个或多个相关的用户业务容器。pod有二种类型：普通的pod和静态pod（比较特殊，他不存放在etcd存储里，而是放在某个node上的一个具体文件中，并且只在此node上运行。）普通的pod一旦被创建就会放在etcd中存储，随后会被Master调度到某个的node上进行绑定，然后该pod被kubelet进程实例化成一组相关的docker容器，并启动起来。

kubectl run  命令可以创建pod，但并不支持所有的功能。其实 kubectl run  并不是直接创建一个 Pod，而是先创建一个 Deployment 资源（replicas=1），再由与 Deployment 关联的 ReplicaSet 来自动创建 Pod，这等价于这样yaml文件 这样的配置。

在 Kubernetes 中，最常使用的是 yaml 文件定义资源，并通过  kubectlcreate -f file.yaml 来创建资源，简单的nginx Pod 定义文件如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginxs
    ports:
    - containerPort: 80
    
#---------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  ports:
  - port: 8078
    name: http
    targetPort: 80
    protocol: TCP
  selector:
    app: nginx
```

==pod的资源限制==：Kubernetes 通过 cgroups 提供容器资源管理的功能，可以限制每个容器的 CPU 和内存使用

```bash
#通过下面的命令限制 nginx 容器最多只用50% 的 CPU 和 128MB 的内存
kubectl set resources deployment nginx-app -c=nginx --limits=cpu=500m,memory=128Mi

#这等同于在每个 Pod 中设置 resources limits
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  containers:
    - image: nginx
      name: nginx
      resources:
        limits:
          cpu: "500m"
          memory: "128Mi"
```

==健康检查==：Kubernetes 提供了两种探针（Probe，支持 exec、tcpSocket 和 http 方式）来探测容器的状态：

- LivenessProbe：探测应用是否处于健康状态，如果不健康则删除并重新创建容器
- ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则不会接收来自 Kubernetes Service 的流量

```yaml
#如下增加了健康检查的部分
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: http
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        resources:
          limits:
            cpu: "500m"
            memory: "128Mi"
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 15
          timeoutSeconds: 1
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          timeoutSeconds: 1
```

==天翼云环境pod的yaml==

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: '9'
  labels:
    ownerName: cpc1mnt
    system_cluster: prod-bss-cluster2
    system_namespace: prod-cpc1-sys
    system_serviceUnit: cpc1gateway
    system_workload: cpc1gateway
  name: cpc1gateway
  namespace: prod-cpc1-sys
spec:
  minReadySeconds: 180
  replicas: 4
  selector:
    matchLabels:
      ownerName: cpc1mnt
      system_cluster: prod-bss-cluster2
      system_namespace: prod-cpc1-sys
      system_serviceUnit: cpc1gateway
      system_workload: cpc1gateway
  template:
    metadata:
      labels:
        ownerName: cpc1mnt
        system_cluster: prod-bss-cluster2
        system_namespace: prod-cpc1-sys
        system_serviceUnit: cpc1gateway
        system_workload: cpc1gateway
    spec:
      containers:
      - env:
        - name: shtelpaas_center_name
          value: cpc1
        - name: pinpoint_applicationName
          value: cpc1gateway
        - name: shtelpaas_app_name
          value: cpc1gateway
        - name: shtelpaas_app_nameserver
          value: http://nameserver1:8761/eureka/,http://nameserver2:8761/eureka/,http://nameserver3:8761/eureka/
        - name: shtelpaas_log_profile
          value: log$prod$1.0
        - name: profiler_shtelpaas_pp_enable
          value: 'true'
        - name: shtelpaas_app_basepkg
          value: com.shtel.paas.service.gateway
        - name: JAVA_OPTS
          value: -Xms5440m -Xmx5440m -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions -XX:NewRatio=2 -XX:ParallelGCThreads=2 -XX:CICompilerCount=2 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:InitialCodeCacheSize=128m -XX:ReservedCodeCacheSize=256m -XX:MinMetaspaceFreeRatio=20 -XX:MaxMetaspaceFreeRatio=80 -XX:MetaspaceSize=768m -XX:MaxMetaspaceSize=768m -XX:CompressedClassSpaceSize=512m  -XX:MaxDirectMemorySize=512M -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:/usr/local/gc.txt -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1 -XX:+LogVMOutput -XX:LogFile=/usr/local/vm.txt -XX:-UseBiasedLocking -XX:+LogCompilation -XX:+SafepointTimeout -XX:SafepointTimeoutDelay=500
        - name: shtelpaas_app_config_profile
          value: app$prod$1.0
        - name: profiler_collector_ip
          value: 10.145.168.243
        image: hub.paas/prod-cpc1-sys/cpc1gateway:shtel-paas-service-gateway-1.0.3-SNAPSHOT-paaspkg.tar-cpc1gateway-1138468312786543441-8
        imagePullPolicy: Always
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8888
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 10
        name: cpc1gateway
        resources:
          limits:
            cpu: 8192m
            memory: 8Gi
          requests:
            cpu: 512m
            memory: 8Gi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /usr/local/gateway-service/log/
          name: app-log-centos
      - env:
        - name: CE_CLUSTER_NAME
          value: prod-bss-cluster2
        - name: CE_NAMESPACE_NAME
          value: prod-cpc1-sys
        - name: CE_WORKLOAD_NAME
          value: cpc1gateway
        - name: CE_SERVICE_NAME
          value: cpc1gateway
        - name: CE_LOG_PATH
          value: /usr/local/gateway-service/log/*.log
        - name: CE_LOG_SERVER_ADDR
          value: 10.145.168.54:6230,10.145.168.55:6230,10.145.168.56:6230
        image: hub.paas/paas/filebeat:0.6
        imagePullPolicy: Always
        name: file-beat
        resources:
          limits:
            cpu: 50m
            memory: 200Mi
          requests:
            cpu: 10m
            memory: 40Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /usr/local/gateway-service/log/
          name: app-log-centos
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: app-log-centos
```

**==Label==** 是识别 Kubernetes 对象的标签， 是一个用户自定义的key=value的键值对 key 最长不能超过 63 字节，value 可以为空，也可以是不超过 253 字节的字符串），可以使用到各种资源对象上，（node,pod,service,RC等）

**==Replication Controller==** 简称RC(复制控制器)，定义了一个期望的场景，即声明某种pod的副本数量在任意时刻都符合某个预期值，(kubernetes始终保证整个集群有【 RC定义的 replicas 数量】pod运行着 )所有RC的定义包括以下几个部分

pod的数量（replicas），用于筛选目标pod的label selector , 当pod的副本数量小于预期数量时，用于创建新pod的pod模板。

下面是一个完整的RC定义例子,

```yaml
apiVersion:v1
kind:ReplicationController
metadata:
  name:frontend
spec:
  replicas:1
  selector:
    tier:frontend
  template:
    metadata:
      labels:
        app:app-demo
        tier:frontend
    spec:
      containers:
      - name:tomcat-demo
        image:tomcat
        imagePullPolicy:IfNotPresent
        env:
        - name:GET_HOSTS_FROM
          value:dns
        ports:
        - containerPort:80
```

当我们把RC提交到kubernetes集群后，master上的Controller Manager会定期巡检当前存活的pod，并保障pod实例数等于RC的期望值。在运行时我们可以通过修RC，来实现对pod实例数的修改 kubectl scale rc redis-slave --replicas=3 ，

新版的kubernetes出现了replica set,可以称为下一次的RC，和Replication Controller区别是支持基于集合的Lable selector，而Replication Controller只支持等式的Lable Selector，但他主要被Deployment这个资源对象所使用，从而形成一套pod创建，删除 更新的编排机制。

**==Deployment==** 相当于RC的一次升级，为了更好的解决pod的编排问题。在内部是使用了replica set来实现的。他的定义和Replica Set的定义很类似。示例如下

```dockerfile
apiVersion: extensions/vlbetal
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
  matchLabels:
    tier: frontend
  matchExpressions:
    - {key: tier, operator: In, values: [frontend]}
template:
  metadata:
    labels:
      app: app-demo
      tier: frontend
  spec:
    containers:
    - name: tomcat-demo
      image: tomcat
      imagePullPolicy: IfNotPresent
      ports:
      - containerPort: 8080
      
# 运行下面命令创建Deployment
kubectl create -f tomcat-deployment.yaml
kubectl get deployments		#查看deployment的信息
#显示内容解释：DESIRED:pod数量的期望值，CURRENT:当前replica的值，UP-TO-DATA：最新版本的pod数量，AVAILBLE:集群中存活的pod数量
kubectl get rs	#查看replica set信息
kubectl describe deployments	#可以看到deployment控制pod的水平扩展过程。
```

**==Horizontal Pod Autoscaler==** pod只能扩容简称HPA，通过追踪分析RC控制的所有pod的负载变化情况，来确定是否需要针对性的调整目标的pod数量。有二种方式作为pod负载的度量指标，CPUUtilizationPercentage和应用程序自定义的度量指标，比如服务在每秒的请求数(TPS或QPS)。

```bash
kubectl autoscale deployment php-apache --cpu-percent=90 --min=1 --max=10	#除了yaml文件定义外，通过命令创建HPA资源对象
```

**==StatefulSet==** 在kubernetes里，pod的管理对象RC、Deployment、DaemonSet和job都是面向无状态的服务，但现实中有很多服务是有状态的。staefulset可以看作是Deployment/rc的一个特殊变种，有以下特性：1、每个pod都有稳定和唯一的网络标识，可以发现集群内的其他成员/2、pod的启停是可以控制的，3、采用了稳定的持久化存储卷，删除pod不会删除与statefulset相关的存储卷。

**==service==** 就是指我们的微服务，通过 labels 为应用提供负载均衡和服务发现，匹配 labels的 Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些endpoints 上，每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。

kubernetes里有三种IP。

- Node IP : node节点的IP地址，集群中每个节点的物理网卡的IP地址。

- Pod IP: pod的ip地址 ，他是docker0网桥分配的IP，是一个虚拟的二层网络，真实的流量都是通过 node IP的物理网卡流出的。

- Cluster IP :service的IP地址，也是一个虚拟的IP，属于kubernetes集群内部的地址，无法再集群直接使用。仅仅作为kubernetes service 这个对象，无法ping，不具备TCP/IP通信的基础，只属于kubernetes集群这样的一个封闭空间，如果需要和Cluster IP 网通信，一般是采用NodePort，因为Cluster IP是kubernetes自己设计的路由规则。

在 kubernetes 中，Pod 的 IP 地址会随着 Pod 的重启而变化，所以不建议直接拿 Pod 的 IP 来交互，service 为一组 Pod（通过 labels 来选择）提供一个统一的入口，并为它们提供负载均衡和自动服务发现。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  ports:
  - port: 8078 # the port that this service should serve on
    name: http
# the container on each pod to connect to, can be a name
# (e.g. 'www') or a number (e.g. 80)
    targetPort: 80
    protocol: TCP
  selector:
    app: nginx
```

**==Volume==** 是pod中能够被多个容器访问的共享目录，用途和目的与docker的容器卷类似，单不能等价。kubernetes提供了很多Volume类型

- 1、emptyDir	是pod分配到node时创建的，当pod从node上移除时，emptyDir中的数据也会被永久被删除
- 2、hostPath	是pod在挂载宿主机上的文件或目录，通常用于，容器的应用日志需要保存时，可以使用宿主机存储，需要访问宿主机上docker引擎内部数据结构的容器应用时，可以定义hostPath为宿主机/var/lib/docker目录，使容器内部可以直接访问docker的文件系统。

- 3、gcePersistentDisk	这样的类型表示使用谷歌公有云提供的永久磁盘。

- 4、awsElasticBlockStore	表示使用了亚马逊提供的 EBS Volume存储数据
- 5、NFS	使用NFS网络文件系统提供的共享目录存储数据
- 6、其他如：iscsi		flocker		rbd		gitRepo

redis 容器指定一个 hostPath 来存储 redis 数据

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-persistent-storage
      mountPath: /data/redis
  volumes:
    - name: redis-persistent-storage
    hostPath:
    path: /data/
```

**==Persistent Volume==** 可以理解成 kubernetes集群中的某个网络存储对应的一块存储。与Volume是有区别的，PV是网络存储，不属于NOde，但可以在每个node上访问，他不是定义在pod上的，而是独立于pod之外，

**==namespace==**(命名空间) 是非常重要的感念，是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。多数情况下用于实现多租户的资源隔离，默认kubernetes启动后，会创建一个名为default的命名空间。常见的 pods, services, replication controllers 和 deployments等都是属于某一个 namespace 的（默认是 default），而 node, persistentVolumes 等则不属于任何 namespace。

**==Annotation==**(注解) 与label用法类似，不同的是label是具有严格的命名规则，而Annotation则是用户任意定义的附加信息。



## 2、深入了解Pod

**pod定义文件模板中个属性说明**

![1.jpg](https://i.loli.net/2019/08/02/5d43aa4f61abd50819.jpg)
![2.jpg](https://i.loli.net/2019/08/02/5d43aa501306b29742.jpg)
![4.jpg](https://i.loli.net/2019/08/02/5d43aa50184d110244.jpg)
![3.jpg](https://i.loli.net/2019/08/02/5d43aa5022eea60864.jpg)

### 2.1、pod的基本用法

在docker中是使用 docker run命令 创建并启动容器的，在kubernetes中允许容器需要主程序一直在前台执行。对于无法使用前台执行的应用，可以使用开源工具supervisor辅助进行前台运行的功能。supervisor提供了一种可以同时启动多个后台应用，并保持supervisor自身在前台执行的机制，可以满足kubernetes对容器的启动要求。

pod可以由1个或者多个容器组成，下面图中就是2个容器，


![5.png](https://i.loli.net/2019/08/02/5d43da00a026c22231.png)

通过 kubectl create -f localredis.yaml 命令创建pod，通过kubectl get pods 查看已创建的pod，可以看到READY信息为2/2，表示pod里有2个容器在运行

### 2.2、静态pod

静态pod是由kubelet创建并进行管理的，仅存在特定的node上的pod，并且总是在kubelet所在的node上运行，他们不能通api service 进行管理，无法与replicationController deployment或daemonset进行关联，并且kubelet无法对他们进行监控检查。

创建静态pod有2种方式：配置文件和http方式

**1、配置文件方式**

需要设置kubelet的启动参数 --config 然后指定kubelet需要监控的配置文件目录，配置启动参数：--config=/etc/kubelet.d/，kubelet会定期扫描该目录，并根据目录中的。yaml和。json文件进行创建操作。

在master节点上使用命令kubectl delete 删除，只会将状态变成为 Pending，pod还是会存在的，所以删除，只能在所在的node机器上，将定义的yaml 从配置文件目录里删除。

**2、http方式**

通过设置kubelet 的启动参数 --manifest-url，kubelet会定期从该url地址下载pod的定义文件，然后创建pod。

### 2.3、pod容器共享Volume

在同一个pod中的多个容器能够共享pod级别的存储卷，存储卷可以被定义为各种类型，多个容器各自进行挂载操作，将一个存储卷挂载为容器内部需要的目录
![容器](https://s2.ax1x.com/2019/08/03/esCJVx.png)

### 2.4、pod的管理配置

应用部署的最佳实践方式，是将所需的配置信息与程序进行分离，这样应用程序就可以更好的复用，通过不同的配置实现灵活的功能。将应用程序打包为容器镜像后，可以通过环境变量或外挂文件的方式挂载配置信息，kubernetes提供了一个统一的应用配置方案叫  configMap。

configMap供容器使用的典型用法：

- 生产为容器的内部环境变量
- 设置容器启动命令的启动参考
- 以存储卷的形式挂载为容器内部的文件或目录

可以用yaml配置文件或者直接使用命令 kubectl create configmap 的方式来创建configMap

**创建configMap资源对象**

**1、通过yaml配置文件方式创建**，参考图片


![5.png](https://i.loli.net/2019/08/03/zan3ql1mFVT8LEG.png)

**2、通过kubectl命令创建**

不使用yaml文件，直接通过kubectl create configmap 也可以创建configmap，可以使用参考 --from-file 或 --from-literal指定内容，并且可以在一行命令里指定多个参数

```bash
#1.通过--from-file参数从文件中创建可以指定key的名称，也可以使用命令行创建多个包含key的configMap
kubectl create configmap NAME --from-file=[key=]source --from-file=[key=]source
#如，当前目录下有配置文件server.xml可以创建一个包含该文件内容的configMap
kubectl create configmap cm-service.xml --from-file=server.xml

#2.通过--from-file参数从目录中创建，该目录下的每个配置文件名都被设置为key，文件的内容被设置为value
kubectl create configmap NAME --from-file=config-files-dir
#如：configfiles目录下有2个配置文件servier.xml和logging.properties，创建一个包含这2个文件的configmap
kubectl create configmap cm-appconf --from-file=configfiles

#3.--from-literal从文件中进行创建，直接将指定的key=value创建为configMap的内容
kubectl create configmap NAME --from-literal=key1=value2 --from-literal=key2=value2
#如:
kubectl create configmap cm-appenv --from-literal=loglevel=info --from-literal=appdatadir=/var/data
```

容器应用对configmap的使用有2种方法：

- 通过环境变量获取configmap中的内容
- 通过volume挂载的方式将configMap中的内容挂载为容器内部文件或目录



**3、在pod中使用configmap**

通过环境变量方式使用configmap

![6.png](E:\Project\MyNotes\GitNote\img\V9UTp4CdEcAXHak.png)

kubernetes还有一个新字段envfrom，实现在pod环境内将configmap中所有定义的key=value自动生成为环境变量

![7.png](E:\Project\MyNotes\GitNote\img\rbV4GEUanCgWuZx.png)

通过volume mount使用configmap

![8.png](E:\Project\MyNotes\GitNote\img\pMkO8hiDy7r3LFP.png)

创建pod后然后登陆容器，查看到configfiles目录下存在server.xml和logging.properties文件

![9.png](E:\Project\MyNotes\GitNote\img\QHqMxCDkVrO8L6d.png)

**4、使用configmap的限制条件**

- configmap必须在pod之前创建
- configmap受namespace限制，只有处于相同的namespaces中的pod才可以引用
- configMap中的配额管理还未实现
- kubelet只支持可以被api server管理的pod使用configMap。kubelet在本node上通过，--manifest-url或--config自动创建的静态pod将无法引用configmap
- 在pod对configMap进行挂载操作时，容器内部职能挂载为目录，无法挂载为文件，如果该目录下还有其他文件，则容器内的目录将会被挂载的configMap覆盖。

**5、在容器内获取pod信息**

每个pod创建出来之后，都会被系统分配唯一的名字，IP地址，并处于某个namespace中，使用 downward API可以获取pod容器内的重要信息。

downwardAPI 可以通过两种方式将pod信息注入容器内部，

- 环境变量：用于单个变量，将pod信息和container信息注入容器内部
- volume挂载：将数组类信息生产文件，挂载到容器内部

集群中的每个节点都需要将自身的标识（ID）及进程绑定的IP地址等信息事先写入配置文件中，进程启动时读取这些信息，然后发布到某个注册中心上，以实现集群节点的自动发现功能，此时downward API 就可以派上用场了，具体的做法是，编写了一个预启动脚本或init container，通过环境变量或文件方式获取POD自身的名称，IP地址等信息，然后写入 主程序的配置文件中，最后启动主程序。

环境变量方式--将pod信息注入为环境信息

通过downwardAPI 将pod的IP，名称和所在namespace注入容器的环境变量中，容器应用使用env命令将全部环境变量打印到标准输出中：

![1.png](https://i.loli.net/2019/08/04/fShFB6MaNzrWIAp.png)

环境变量方式：将容器资源信息注入为环境变量

下例通过downward API 将 container的资源请求和限制请求信息注入容器的环境变量中，容器应用使用printenv命令将设置的资源请求和资源限制环境变量打印到标准输出中。（参考kubernetes权威指南 139页）

**6、pod生命周期和重启策略**

pod的状态：

- pending：API server 已经创建该pod。但pod内还有一个或多个容器没有被创建。

- Running：pod内所有容器均已创建，且至少有一个容器在运行中。

- succeeded：pod内所有容器均成功执行退出，且不会在重启。

- Failed：pod内所有容器均已退出，但至少有一个容器为失败状态

- Unknown：由于某种原因无法获取该pod的状态，可能由于网络通信不畅导致。


pod的重启策略应用于pod内的所有容器，并且仅在pod所处的node上，由kubelet进行判断和重启操作。当某个容器异常退出或健康检查失败时，kubelet将根据重启策略的设置来进行相应的操作

pod的重启策略包括always、onfailure和Never，默认值为always

- Always：当容器失效时，有kubelet自动重启该容器。

- OnFailure：当容器终止运行且退出码不为0时，由kubelet自动重启该容器。

- Never：不论容器运行状态如何，kubelet都不会重启该容器


![2.png](E:\Project\MyNotes\GitNote\img\asAJf1dm5SyhXeB.png)

**7、pod健康检查**

对pod的健康检查可以通过两类探针来检查，LIvenessProbe和ReadinessProbe

LIvenessProbe探针：用于排断容器是否存活（running状态）如果LivenessProbe探针探测到容器不健康，kubelet则将杀死该容器，并根据容器的重启策略做相应的处理，如果一个容器不包含LivenessProbe探针，那么kubelet认为该容器的livenessProbe探针返回的是 success

ReadinessProbe探针：用于判断容器是否启动完成（ready状态），可以接收请求，如果ReadinessProbe探针检测到失败，则pod的状态将被修改，Endpoint Controller将从service的Endpoint中删除包含该容器所在pod的Endpoint。

kubelet定期执行livenessProbe探针来诊断容器的健康状态，livenessProbe有三种实现方式：

- ExecAction：在容器内部执行一个命令，如果该命令返回码为0 表示容器健康

- TCPSocketAction：通过容器的IP地址和端口号执行TCP检查，如果能够建立TCP连接，表明容器健康

- HTTPGetAction：通过容器的IP地址，端口号及路径调用http get方法，如果响应的状态码大于等于200且小于400，则认为容器状态健康。


对应每种探测方式，都需要设置initialDelaySeconds和timeoutSeconds两个参数，他们的含义分别如下：

initialDelaySeconds：启动容器后进行首次健康检查的等待时间，单位 为秒

timeoutSeconds：健康检查的发送请求后等待响应的超时时间，单位为秒，当超时发生时，kubelet会认为容器已经无法提供服务，将会重启该容器。

**8、玩转pod调度**

pod在大部分场景下都只是个容器的载体，通常需要通过deployment、DaemonSet、RC、job等对象来完成一组pod的调度与自动控制功能。

**Deployment/RC：全自动调度**，主要功能之一就是自动部署一个容器的应用，以及监控实例的数量，（字段 replicas 是设置实例的个数）在集群内始终维持用户指定的副本数量。pod由系统自动完成调度，他们各自最终运行在那个节点上，是有scheduler经过一系列算法计算出来的，用户无法干涉。

**nodeSelector：定向调度**，kube-scheduler 负责实现pod的调度，整个调度过程，通过执行一系列的算法，最终为每个pod计算出一个最佳的目标节点，这一过程是自动完成的，通常我们无法知道pod最终会被调度到那个节点上。也可以人工干预，通过node的标签（label）和pod的nodeSelector属性相匹配，达到目的。

```bash
kubectl label nodes <node-name> <label-key>=<label-value>	#1.首先通过kubectl label 命令给目标node打上一些标签
#1.例如：为k8s-node-1节点打上一个zone=north的标签
kubectl label nodes k8s-node-1 zone=north	#也可以通过修改资源定义文件并执行命令完成kubectl replace -f xxx.yaml	
#2.然后在pod的定义中加上nodeSelector,参考定义方式图如下,执行命令kubectl create -f 创建pod，就会调度到拥有zone=north标签的node上
```

![1.png](E:\Project\MyNotes\GitNote\img\osICaSNEyKMXJLc.png)

![ehlCQK.png](https://s2.ax1x.com/2019/08/06/ehlCQK.png)

**nodeAffinity：node亲和性调度策略**，是用于替换nodeSelector的全新调度策略，目前有二种节点亲和性表达：（参考kubernetes权威指南148页）

- RequiredDuringSchedulingIgnoredDuringExecution：必须满足指定的规则才可以调度pod到node上，相当于硬限制
- PreferredDuringSchedulingIgnoredDuringExecution：强调优先满足指定规则，调度器会尝试调度pod到node上，但并不强求，相当于软限制，多个优先级规则还可以设置权重值（weight）

**podAffinity：pod亲和与互斥调度策略**

**Taints和Tolerations（污点和容忍）**：taint需要和toleration配合使用，让pod避开那些不适合的node。

```bash
kubectl taint nodes node1 key=value:NoSchedule	#为node设置Taint信息
```







## 3、深入了解service





## 4、kubernetes API server 原理





## 5、Controller Manager原理





## 6、Scheduler原理







## 7、kubelet运行机制







## 8、kube-proxy运行机制





## 9、网络原理







## 10、共享存储原理





